{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-diB63UzmzWj",
        "outputId": "be15fe8c-2688-4838-e653-58985ecd5ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.24.7)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (10.4.0)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (1.16.0)\n",
            "Collecting timm==0.9.7 (from segmentation-models-pytorch)\n",
            "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.4.1+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16425 sha256=cc932f091d65db7a344ba0781b82ab0f892df64a52de607f0997361dd1e76caf\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=c4702f42b0fdf0b904f3735fb3eda8dd4b89e036085ada4016fdc8aa1a0beaf1\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.15)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.24.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore>=0.0.15 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.16)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (10.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch\n",
        "!pip install albumentations\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch pycocotools\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooXiu49gm-Hs",
        "outputId": "191b1ad1-036f-4cd6-ecde-b9c4f7295428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.7.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.24.7)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (10.4.0)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.7.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (1.16.0)\n",
            "Requirement already satisfied: timm==0.9.7 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.9.7)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.4.1+cu121)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import segmentation_models_pytorch as smp\n",
        "from pycocotools.coco import COCO\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "AobBj63ZnrLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def merge_coco_json(json_files, output_file):\n",
        "    merged_data = {\n",
        "        \"images\": [],\n",
        "        \"annotations\": [],\n",
        "        \"categories\": []\n",
        "    }\n",
        "\n",
        "    # 用于确保每个类别只添加一次\n",
        "    category_ids = set()\n",
        "\n",
        "    for json_file in json_files:\n",
        "        with open(json_file, 'r') as f:\n",
        "            coco_data = json.load(f)\n",
        "            merged_data['images'].extend(coco_data['images'])\n",
        "            merged_data['annotations'].extend(coco_data['annotations'])\n",
        "\n",
        "            for category in coco_data['categories']:\n",
        "                if category['id'] not in category_ids:\n",
        "                    merged_data['categories'].append(category)\n",
        "                    category_ids.add(category['id'])\n",
        "\n",
        "    # 将合并后的数据写入到一个新的 JSON 文件中\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(merged_data, f)\n",
        "\n",
        "# 示例用法\n",
        "json_files = ['/content/drive/MyDrive/onlyvirus/labels/train/EPSON008 (0_0).json', '/content/drive/MyDrive/onlyvirus/labels/train/EPSON008 (0_1).json', '/content/drive/MyDrive/onlyvirus/labels/train/EPSON008 (1_0).json','/content/drive/MyDrive/onlyvirus/labels/train/EPSON008 (1_1).json','/content/drive/MyDrive/onlyvirus/labels/train/EPSON008 (2_0).json','/content/drive/MyDrive/onlyvirus/labels/train/EPSON008 (2_1).json']\n",
        "merge_coco_json(json_files, 'only_merged_coco_annotations.json')\n",
        "\n"
      ],
      "metadata": {
        "id": "xe--zLrcsps5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Zg_qJjmr5rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class CocoSegmentationDataset(Dataset):\n",
        "    def __init__(self, annotation_file, images_dir, transform=None):\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            self.coco_data = json.load(f)\n",
        "\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "        self.image_ids = [image['id'] for image in self.coco_data['images']]\n",
        "        self.masks = self.generate_masks()\n",
        "\n",
        "    def generate_masks(self):\n",
        "        masks = []\n",
        "        for image in self.coco_data['images']:\n",
        "            mask = np.zeros((image['height'], image['width']), dtype=np.uint8)\n",
        "            for annotation in self.coco_data['annotations']:\n",
        "                if annotation['image_id'] == image['id']:\n",
        "                    cv2.fillPoly(mask, [np.array(annotation['segmentation'][0]).reshape(-1, 2).astype(np.int32)], 1)\n",
        "            masks.append(mask)\n",
        "        return masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.image_ids[idx]\n",
        "        image_info = next(item for item in self.coco_data['images'] if item['id'] == image_id)\n",
        "        img_path = os.path.join(self.images_dir, image_info['file_name'])\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        mask = self.masks[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        return image, mask\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as A\n",
        "\n",
        "# 定义数据增强\n",
        "transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.HorizontalFlip(),\n",
        "    A.VerticalFlip(),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    A.pytorch.transforms.ToTensorV2()\n",
        "])\n",
        "\n",
        "# 加载数据集\n",
        "train_dataset = CocoSegmentationDataset(\n",
        "    annotation_file='merged_coco_annotations.json',\n",
        "    images_dir='/content/drive/MyDrive/onlyvirus/images/train',\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "val_dataset = CocoSegmentationDataset(\n",
        "    annotation_file='/content/drive/MyDrive/virusdata/labels/val/EPSON008 (3_0).json',\n",
        "    images_dir='/content/drive/MyDrive/virusdata/images/val',\n",
        "    transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "import torch.optim as optim\n",
        "\n",
        "# 初始化模型\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = smp.DeepLabV3(encoder_name='resnet50', encoder_weights='imagenet', classes=1, activation='sigmoid')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# 设置损失函数和优化器\n",
        "criterion= torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)  # 添加L2正则化\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# 定义函数来计算精确度、召回率和F1分数\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    # 将Tensor转换为NumPy数组并展平\n",
        "    y_true_flat = y_true.view(-1).cpu().numpy()\n",
        "    y_pred_flat = y_pred.view(-1).cpu().numpy()\n",
        "\n",
        "    precision = precision_score(y_true_flat, y_pred_flat)\n",
        "    recall = recall_score(y_true_flat, y_pred_flat)\n",
        "    f1 = f1_score(y_true_flat, y_pred_flat)\n",
        "\n",
        "    return precision, recall, f1\n",
        "def visualize_results(image, mask, prediction):\n",
        "    \"\"\"可视化原图、真实掩模和预测掩模\"\"\"\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # 转换图像以便可视化\n",
        "    image = image.permute(1, 2, 0).cpu().numpy()  # 从 (C, H, W) 转为 (H, W, C)\n",
        "    if image.shape[-1] == 1:  # 如果是单通道图像，转换为灰度图\n",
        "        image = image[..., 0]\n",
        "\n",
        "    # 处理掩模和预测\n",
        "    mask = mask.squeeze().cpu().numpy()  # 去掉多余的通道维度\n",
        "    prediction = prediction.squeeze().cpu().numpy()  # 去掉多余的通道维度\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(image, cmap='gray' if image.ndim == 2 else None)  # 如果是灰度图，使用 gray\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.title('Ground Truth Mask')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(prediction, cmap='gray')\n",
        "    plt.title('Predicted Mask')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 在验证阶段调用可视化函数\n",
        "\n",
        "\n",
        "# 训练循环\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, masks in train_loader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, masks.unsqueeze(1))  # 对于二分类\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # 验证阶段\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_precision = 0\n",
        "        total_recall = 0\n",
        "        total_f1 = 0\n",
        "        count = 0\n",
        "\n",
        "        for val_images, val_masks in val_loader:\n",
        "            val_images = val_images.to(device)\n",
        "            val_masks = val_masks.to(device).float()\n",
        "\n",
        "            val_outputs = model(val_images)\n",
        "            val_predictions = (val_outputs > 0.5).float()  # 二值化输出\n",
        "\n",
        "            # 计算精确度、召回率和F1分数\n",
        "            precision = precision_score(val_masks.view(-1).cpu().numpy(), val_predictions.view(-1).cpu().numpy())\n",
        "            recall = recall_score(val_masks.view(-1).cpu().numpy(), val_predictions.view(-1).cpu().numpy())\n",
        "            f1 = f1_score(val_masks.view(-1).cpu().numpy(), val_predictions.view(-1).cpu().numpy())\n",
        "\n",
        "            total_precision += precision\n",
        "            total_recall += recall\n",
        "            total_f1 += f1\n",
        "            count += 1\n",
        "\n",
        "            # 可视化结果\n",
        "            # if count == 1:  # 只可视化一次\n",
        "            #     visualize_results(val_images[0], val_masks[0], val_predictions[0])\n",
        "        avg_precision = total_precision / count\n",
        "        avg_recall = total_recall / count\n",
        "        avg_f1 = total_f1 / count\n",
        "\n",
        "        print(f\"Validation - Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, F1 Score: {avg_f1:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaH46MuIpBm4",
        "outputId": "51c32ebc-07b1-407a-cf8f-a98b5c42bb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.7886\n",
            "Validation - Precision: 0.1458, Recall: 0.8814, F1 Score: 0.2503\n",
            "Epoch [2/100], Loss: 0.7167\n",
            "Validation - Precision: 0.1524, Recall: 1.0000, F1 Score: 0.2645\n",
            "Epoch [3/100], Loss: 0.7171\n",
            "Validation - Precision: 0.2368, Recall: 0.8424, F1 Score: 0.3697\n",
            "Epoch [4/100], Loss: 0.7224\n",
            "Validation - Precision: 0.2083, Recall: 0.6163, F1 Score: 0.3114\n",
            "Epoch [5/100], Loss: 0.7078\n",
            "Validation - Precision: 0.2303, Recall: 0.4272, F1 Score: 0.2993\n",
            "Epoch [6/100], Loss: 0.8079\n",
            "Validation - Precision: 0.1600, Recall: 0.1328, F1 Score: 0.1451\n",
            "Epoch [7/100], Loss: 0.7807\n",
            "Validation - Precision: 0.2760, Recall: 0.4712, F1 Score: 0.3481\n",
            "Epoch [8/100], Loss: 0.7733\n",
            "Validation - Precision: 0.4300, Recall: 0.4638, F1 Score: 0.4463\n",
            "Epoch [9/100], Loss: 0.7699\n",
            "Validation - Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.7494\n",
            "Validation - Precision: 0.4938, Recall: 0.2803, F1 Score: 0.3576\n",
            "Epoch [11/100], Loss: 0.6652\n",
            "Validation - Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
            "Epoch [12/100], Loss: 0.7328\n",
            "Validation - Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/100], Loss: 0.6446\n",
            "Validation - Precision: 0.7620, Recall: 0.0564, F1 Score: 0.1050\n",
            "Epoch [14/100], Loss: 0.6497\n",
            "Validation - Precision: 0.5824, Recall: 0.4880, F1 Score: 0.5310\n",
            "Epoch [15/100], Loss: 0.7079\n",
            "Validation - Precision: 0.5722, Recall: 0.4397, F1 Score: 0.4973\n",
            "Epoch [16/100], Loss: 0.6287\n",
            "Validation - Precision: 0.6226, Recall: 0.8167, F1 Score: 0.7066\n",
            "Epoch [17/100], Loss: 0.7046\n",
            "Validation - Precision: 0.5404, Recall: 0.7961, F1 Score: 0.6438\n",
            "Epoch [18/100], Loss: 0.6912\n",
            "Validation - Precision: 0.5339, Recall: 0.8344, F1 Score: 0.6512\n",
            "Epoch [19/100], Loss: 0.7052\n",
            "Validation - Precision: 0.5936, Recall: 0.9316, F1 Score: 0.7252\n",
            "Epoch [20/100], Loss: 0.6770\n",
            "Validation - Precision: 0.6280, Recall: 0.8864, F1 Score: 0.7352\n",
            "Epoch [21/100], Loss: 0.6953\n",
            "Validation - Precision: 0.5414, Recall: 0.5438, F1 Score: 0.5426\n",
            "Epoch [22/100], Loss: 0.6744\n",
            "Validation - Precision: 0.7889, Recall: 0.3261, F1 Score: 0.4615\n",
            "Epoch [23/100], Loss: 0.6702\n",
            "Validation - Precision: 0.5273, Recall: 0.4827, F1 Score: 0.5040\n",
            "Epoch [24/100], Loss: 0.6410\n",
            "Validation - Precision: 0.7405, Recall: 0.3900, F1 Score: 0.5109\n",
            "Epoch [25/100], Loss: 0.6217\n",
            "Validation - Precision: 0.8620, Recall: 0.4863, F1 Score: 0.6218\n",
            "Epoch [26/100], Loss: 0.6171\n",
            "Validation - Precision: 0.8587, Recall: 0.5338, F1 Score: 0.6583\n",
            "Epoch [27/100], Loss: 0.6148\n",
            "Validation - Precision: 0.9262, Recall: 0.3271, F1 Score: 0.4835\n",
            "Epoch [28/100], Loss: 0.6888\n",
            "Validation - Precision: 0.8983, Recall: 0.2734, F1 Score: 0.4192\n",
            "Epoch [29/100], Loss: 0.6696\n",
            "Validation - Precision: 0.8971, Recall: 0.4137, F1 Score: 0.5663\n",
            "Epoch [30/100], Loss: 0.6078\n",
            "Validation - Precision: 0.8721, Recall: 0.4748, F1 Score: 0.6148\n",
            "Epoch [31/100], Loss: 0.5949\n",
            "Validation - Precision: 0.8904, Recall: 0.4474, F1 Score: 0.5955\n",
            "Epoch [32/100], Loss: 0.6128\n",
            "Validation - Precision: 0.8754, Recall: 0.1341, F1 Score: 0.2326\n",
            "Epoch [33/100], Loss: 0.5869\n",
            "Validation - Precision: 0.9749, Recall: 0.1142, F1 Score: 0.2044\n",
            "Epoch [34/100], Loss: 0.6753\n",
            "Validation - Precision: 0.9749, Recall: 0.1476, F1 Score: 0.2564\n",
            "Epoch [35/100], Loss: 0.6182\n",
            "Validation - Precision: 0.9383, Recall: 0.2197, F1 Score: 0.3560\n",
            "Epoch [36/100], Loss: 0.6107\n",
            "Validation - Precision: 0.9038, Recall: 0.2057, F1 Score: 0.3351\n",
            "Epoch [37/100], Loss: 0.5799\n",
            "Validation - Precision: 0.8266, Recall: 0.2567, F1 Score: 0.3918\n",
            "Epoch [38/100], Loss: 0.6056\n",
            "Validation - Precision: 0.8856, Recall: 0.4074, F1 Score: 0.5580\n",
            "Epoch [39/100], Loss: 0.6808\n",
            "Validation - Precision: 0.8639, Recall: 0.5839, F1 Score: 0.6968\n",
            "Epoch [40/100], Loss: 0.5801\n",
            "Validation - Precision: 0.8619, Recall: 0.5613, F1 Score: 0.6799\n",
            "Epoch [41/100], Loss: 0.6602\n",
            "Validation - Precision: 0.9442, Recall: 0.2487, F1 Score: 0.3936\n",
            "Epoch [42/100], Loss: 0.6405\n",
            "Validation - Precision: 0.9072, Recall: 0.0921, F1 Score: 0.1673\n",
            "Epoch [43/100], Loss: 0.6700\n",
            "Validation - Precision: 0.9227, Recall: 0.1008, F1 Score: 0.1818\n",
            "Epoch [44/100], Loss: 0.6667\n",
            "Validation - Precision: 0.9580, Recall: 0.1673, F1 Score: 0.2848\n",
            "Epoch [45/100], Loss: 0.5738\n",
            "Validation - Precision: 0.8912, Recall: 0.3359, F1 Score: 0.4879\n",
            "Epoch [46/100], Loss: 0.5951\n",
            "Validation - Precision: 0.8974, Recall: 0.3895, F1 Score: 0.5432\n",
            "Epoch [47/100], Loss: 0.6216\n",
            "Validation - Precision: 0.9335, Recall: 0.4125, F1 Score: 0.5722\n",
            "Epoch [48/100], Loss: 0.5958\n",
            "Validation - Precision: 0.9286, Recall: 0.4152, F1 Score: 0.5738\n",
            "Epoch [49/100], Loss: 0.6228\n",
            "Validation - Precision: 0.9145, Recall: 0.5481, F1 Score: 0.6854\n",
            "Epoch [50/100], Loss: 0.6133\n",
            "Validation - Precision: 0.9292, Recall: 0.3886, F1 Score: 0.5481\n",
            "Epoch [51/100], Loss: 0.5938\n",
            "Validation - Precision: 0.9278, Recall: 0.2184, F1 Score: 0.3536\n",
            "Epoch [52/100], Loss: 0.6579\n",
            "Validation - Precision: 0.9650, Recall: 0.1992, F1 Score: 0.3303\n",
            "Epoch [53/100], Loss: 0.6742\n",
            "Validation - Precision: 0.9390, Recall: 0.3012, F1 Score: 0.4560\n",
            "Epoch [54/100], Loss: 0.6629\n",
            "Validation - Precision: 0.8358, Recall: 0.3101, F1 Score: 0.4523\n",
            "Epoch [55/100], Loss: 0.6004\n",
            "Validation - Precision: 0.8600, Recall: 0.3525, F1 Score: 0.5001\n",
            "Epoch [56/100], Loss: 0.5998\n",
            "Validation - Precision: 0.7938, Recall: 0.2982, F1 Score: 0.4336\n",
            "Epoch [57/100], Loss: 0.6178\n",
            "Validation - Precision: 0.8328, Recall: 0.3998, F1 Score: 0.5402\n",
            "Epoch [58/100], Loss: 0.6477\n",
            "Validation - Precision: 0.7982, Recall: 0.4444, F1 Score: 0.5710\n",
            "Epoch [59/100], Loss: 0.5657\n",
            "Validation - Precision: 0.9073, Recall: 0.3114, F1 Score: 0.4637\n",
            "Epoch [60/100], Loss: 0.6223\n",
            "Validation - Precision: 0.9508, Recall: 0.3430, F1 Score: 0.5041\n",
            "Epoch [61/100], Loss: 0.6703\n",
            "Validation - Precision: 0.9162, Recall: 0.4469, F1 Score: 0.6008\n",
            "Epoch [62/100], Loss: 0.6587\n",
            "Validation - Precision: 0.9196, Recall: 0.3402, F1 Score: 0.4967\n",
            "Epoch [63/100], Loss: 0.6543\n",
            "Validation - Precision: 0.9589, Recall: 0.2912, F1 Score: 0.4468\n",
            "Epoch [64/100], Loss: 0.6620\n",
            "Validation - Precision: 0.9696, Recall: 0.2029, F1 Score: 0.3356\n",
            "Epoch [65/100], Loss: 0.6078\n",
            "Validation - Precision: 0.9597, Recall: 0.1281, F1 Score: 0.2261\n",
            "Epoch [66/100], Loss: 0.6004\n",
            "Validation - Precision: 0.9626, Recall: 0.1008, F1 Score: 0.1825\n",
            "Epoch [67/100], Loss: 0.6585\n",
            "Validation - Precision: 0.9712, Recall: 0.0619, F1 Score: 0.1164\n",
            "Epoch [68/100], Loss: 0.6429\n",
            "Validation - Precision: 0.9772, Recall: 0.1785, F1 Score: 0.3019\n",
            "Epoch [69/100], Loss: 0.5882\n",
            "Validation - Precision: 0.9789, Recall: 0.1701, F1 Score: 0.2898\n",
            "Epoch [70/100], Loss: 0.5975\n",
            "Validation - Precision: 0.9467, Recall: 0.1131, F1 Score: 0.2020\n",
            "Epoch [71/100], Loss: 0.6573\n",
            "Validation - Precision: 0.9751, Recall: 0.1389, F1 Score: 0.2431\n",
            "Epoch [72/100], Loss: 0.6422\n",
            "Validation - Precision: 0.9632, Recall: 0.1888, F1 Score: 0.3157\n",
            "Epoch [73/100], Loss: 0.6046\n",
            "Validation - Precision: 0.9181, Recall: 0.1235, F1 Score: 0.2177\n",
            "Epoch [74/100], Loss: 0.6454\n",
            "Validation - Precision: 0.8872, Recall: 0.0972, F1 Score: 0.1751\n",
            "Epoch [75/100], Loss: 0.5992\n",
            "Validation - Precision: 0.7704, Recall: 0.1236, F1 Score: 0.2130\n",
            "Epoch [76/100], Loss: 0.5960\n",
            "Validation - Precision: 0.7676, Recall: 0.1766, F1 Score: 0.2871\n",
            "Epoch [77/100], Loss: 0.6153\n",
            "Validation - Precision: 0.8739, Recall: 0.4316, F1 Score: 0.5778\n",
            "Epoch [78/100], Loss: 0.6680\n",
            "Validation - Precision: 0.7944, Recall: 0.6421, F1 Score: 0.7102\n",
            "Epoch [79/100], Loss: 0.6149\n",
            "Validation - Precision: 0.7649, Recall: 0.7670, F1 Score: 0.7660\n",
            "Epoch [80/100], Loss: 0.5982\n",
            "Validation - Precision: 0.8976, Recall: 0.6563, F1 Score: 0.7582\n",
            "Epoch [81/100], Loss: 0.6054\n",
            "Validation - Precision: 0.8937, Recall: 0.6624, F1 Score: 0.7608\n",
            "Epoch [82/100], Loss: 0.6190\n",
            "Validation - Precision: 0.9067, Recall: 0.6041, F1 Score: 0.7251\n",
            "Epoch [83/100], Loss: 0.5981\n",
            "Validation - Precision: 0.9202, Recall: 0.5377, F1 Score: 0.6788\n",
            "Epoch [84/100], Loss: 0.6558\n",
            "Validation - Precision: 0.8914, Recall: 0.5493, F1 Score: 0.6797\n",
            "Epoch [85/100], Loss: 0.6145\n",
            "Validation - Precision: 0.9402, Recall: 0.4907, F1 Score: 0.6449\n",
            "Epoch [86/100], Loss: 0.6385\n",
            "Validation - Precision: 0.9224, Recall: 0.6067, F1 Score: 0.7320\n",
            "Epoch [87/100], Loss: 0.6390\n",
            "Validation - Precision: 0.9546, Recall: 0.5048, F1 Score: 0.6604\n",
            "Epoch [88/100], Loss: 0.5866\n",
            "Validation - Precision: 0.9651, Recall: 0.4430, F1 Score: 0.6072\n",
            "Epoch [89/100], Loss: 0.6541\n",
            "Validation - Precision: 0.9591, Recall: 0.2780, F1 Score: 0.4311\n",
            "Epoch [90/100], Loss: 0.6042\n",
            "Validation - Precision: 0.9633, Recall: 0.4496, F1 Score: 0.6130\n",
            "Epoch [91/100], Loss: 0.6386\n",
            "Validation - Precision: 0.9210, Recall: 0.6444, F1 Score: 0.7582\n",
            "Epoch [92/100], Loss: 0.5843\n",
            "Validation - Precision: 0.9279, Recall: 0.6028, F1 Score: 0.7308\n",
            "Epoch [93/100], Loss: 0.6543\n",
            "Validation - Precision: 0.9373, Recall: 0.5286, F1 Score: 0.6760\n",
            "Epoch [94/100], Loss: 0.5826\n",
            "Validation - Precision: 0.9587, Recall: 0.3463, F1 Score: 0.5088\n",
            "Epoch [95/100], Loss: 0.6493\n",
            "Validation - Precision: 0.9162, Recall: 0.3226, F1 Score: 0.4771\n",
            "Epoch [96/100], Loss: 0.6507\n",
            "Validation - Precision: 0.9444, Recall: 0.4638, F1 Score: 0.6221\n",
            "Epoch [97/100], Loss: 0.6532\n",
            "Validation - Precision: 0.8951, Recall: 0.5421, F1 Score: 0.6753\n",
            "Epoch [98/100], Loss: 0.6017\n",
            "Validation - Precision: 0.9039, Recall: 0.6275, F1 Score: 0.7408\n",
            "Epoch [99/100], Loss: 0.6139\n",
            "Validation - Precision: 0.9030, Recall: 0.4750, F1 Score: 0.6226\n",
            "Epoch [100/100], Loss: 0.6133\n",
            "Validation - Precision: 0.8990, Recall: 0.4490, F1 Score: 0.5989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rR1z0xcMiQsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, jaccard_score, f1_score\n",
        "\n",
        "model.eval()  # 设置模型为评估模式\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():  # 禁用梯度计算\n",
        "    for images, targets in test_loader:\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # 前向传播\n",
        "        outputs = model(images)\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).float()  # 将输出转换为二进制掩码\n",
        "\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "# 将所有结果拼接\n",
        "all_preds = np.concatenate(all_preds)\n",
        "all_targets = np.concatenate(all_targets)\n",
        "\n",
        "# 计算性能指标\n",
        "accuracy = accuracy_score(all_targets.flatten(), all_preds.flatten())\n",
        "jaccard = jaccard_score(all_targets.flatten(), all_preds.flatten())\n",
        "dice_coefficient = f1_score(all_targets.flatten(), all_preds.flatten())\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Jaccard Index: {jaccard:.4f}')\n",
        "print(f'Dice Coefficient: {dice_coefficient:.4f}')\n"
      ],
      "metadata": {
        "id": "0cp8_HDzCC7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# 定义简单的FCN网络\n",
        "class SimpleFCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleFCN, self).__init__()\n",
        "        # 第一层卷积，输入尺寸为 (128, 128, 1)，输出尺寸为 (122, 122, 64)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, padding=1)\n",
        "        # 第二层卷积，输入尺寸为 (122, 122, 64)，输出尺寸为 (120, 120, 16)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))  # 应用 ReLU 激活函数\n",
        "        x = F.relu(self.conv2(x))\n",
        "        return x  # 输出形状为 (batch_size, 16, 120, 120)\n",
        "\n",
        "# 初始化模型\n",
        "model = SimpleFCN()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quymdKHsiRgZ",
        "outputId": "0b8b440f-68f6-40ce-8ed4-0699ffc7b346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "# 自定义数据集类\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # 读取灰度图\n",
        "        label = self.labels[idx]  # 对应的标签\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# 定义图像变换\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # 转换为Tensor\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # 标准化\n",
        "])\n",
        "\n",
        "# 假设我们有训练集图像路径和标签列表\n",
        "train_img_paths = ['path_to_img1', 'path_to_img2', ...]  # 训练集图像路径\n",
        "train_labels = ['path_to_mask1', 'path_to_mask2', ...]   # 训练集标签\n",
        "\n",
        "# 创建训练数据集和数据加载器\n",
        "train_dataset = CustomDataset(train_img_paths, train_labels, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n"
      ],
      "metadata": {
        "id": "rCruczBmiW0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def merge_json_files(json_dir):\n",
        "    merged_data = {\"images\": [], \"annotations\": [], \"categories\": []}\n",
        "\n",
        "    for filename in os.listdir(json_dir):\n",
        "        if filename.endswith('.json'):\n",
        "            with open(os.path.join(json_dir, filename), 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "                # 仅添加唯一类别（假设所有文件的类别相同）\n",
        "                for category in data['categories']:\n",
        "                    if category not in merged_data['categories']:\n",
        "                        merged_data['categories'].append(category)\n",
        "\n",
        "                merged_data['images'].extend(data['images'])\n",
        "                merged_data['annotations'].extend(data['annotations'])\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "# 设置 JSON 文件夹路径\n",
        "json_directory = '/content/drive/MyDrive/fcnvirus/labels/'  # 修改为您的路径\n",
        "merged_json = merge_json_files(json_directory)\n",
        "\n",
        "# 将合并后的数据写入新的 JSON 文件\n",
        "with open('merged_annotations.json', 'w') as outfile:\n",
        "    json.dump(merged_json, outfile)\n"
      ],
      "metadata": {
        "id": "wRA6aCVZsCI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageDraw\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class COCOVirusDataset(Dataset):\n",
        "    def __init__(self, image_dir, json_file, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.annotations = []\n",
        "\n",
        "        # 加载单个 JSON 文件中的数据\n",
        "        if os.path.isfile(json_file):\n",
        "            with open(json_file, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                self.images = data['images']\n",
        "                self.annotations = data['annotations']\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"无法找到指定的 JSON 文件: {json_file}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def fill_mask(self, mask, segmentation):\n",
        "        draw = ImageDraw.Draw(mask)\n",
        "        if isinstance(segmentation, list):\n",
        "            for segment in segmentation:\n",
        "                polygon = [(segment[i], segment[i + 1]) for i in range(0, len(segment), 2)]\n",
        "                draw.polygon(polygon, outline=1, fill=1)\n",
        "        return mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_info = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, image_info['file_name'])\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            raise FileNotFoundError(f\"图像文件 {img_path} 未找到\")\n",
        "\n",
        "        image = Image.open(img_path).convert(\"L\")\n",
        "\n",
        "        # 创建一个空白的掩码\n",
        "        mask = Image.new(\"L\", (image_info['width'], image_info['height']), 0)\n",
        "\n",
        "        for ann in self.annotations:\n",
        "            if ann['image_id'] == image_info['id']:\n",
        "                segmentation = ann.get('segmentation', [])\n",
        "                if segmentation:\n",
        "                    mask = self.fill_mask(mask, segmentation)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        # 确保掩码的形状是 [1, height, width]\n",
        "        mask = mask.squeeze(0)  # 去掉不必要的维度\n",
        "        return image, mask.unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "image_dir = '/content/drive/MyDrive/onlyvirus35/images'\n",
        "json_files ='/content/drive/MyDrive/onlyvirus35/json/onlyvirus-2.json'\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # 将图像和掩码调整为相同大小\n",
        "    transforms.ToTensor()  # 转换为张量\n",
        "])\n",
        "\n",
        "dataset = COCOVirusDataset(image_dir, json_files, transform)\n",
        "train_size = int(0.6 * len(dataset))  # 80% 作为训练集\n",
        "val_size = len(dataset) - train_size  # 20% 作为验证集\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "# 定义简单 FCN 模型（与之前相同）\n",
        "class SimpleFCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleFCN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, padding=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1)  # 输出通道为1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "          # 输出形状为 [batch_size, 1, height, width]\n",
        "         # 去掉通道维度，输出形状为 [batch_size, height, width]\n",
        "\n",
        "        return torch.sigmoid(x)  # 使用sigmoid函数生成概率图\n",
        "\n",
        "# 训练模型\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SimpleFCN().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "def calculate_metrics(predictions, targets):\n",
        "    \"\"\"\n",
        "    计算精度和召回率\n",
        "    predictions: 模型的输出（概率）\n",
        "    targets: 真实标签\n",
        "    \"\"\"\n",
        "    preds = (predictions > 0.5).float()  # 将概率转换为二进制标签\n",
        "    targets = targets.cpu().numpy()\n",
        "    preds = preds.cpu().numpy()\n",
        "\n",
        "    precision = precision_score(targets.flatten(), preds.flatten(), average='binary')\n",
        "    recall = recall_score(targets.flatten(), preds.flatten(), average='binary')\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=10, device='cuda'):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # 存储所有的真实标签和预测结果\n",
        "        all_targets = []\n",
        "        all_predictions = []\n",
        "\n",
        "        for images, masks in train_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            # 确保输出和掩码的形状一致\n",
        "            if outputs.size() != masks.size():\n",
        "                continue  # 跳过形状不匹配的批次\n",
        "\n",
        "            # 计算损失\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # 保存真实标签和预测结果\n",
        "            all_targets.append(masks)\n",
        "            all_predictions.append(outputs)\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # 确保至少有一个批次的输出被保存\n",
        "        if len(all_targets) == 0:\n",
        "            print(f\"Warning: No targets collected during training epoch {epoch + 1}.\")\n",
        "            continue\n",
        "\n",
        "        # 将所有的真实标签和预测结果拼接在一起\n",
        "        all_targets = torch.cat(all_targets, dim=0)\n",
        "        all_predictions = torch.cat(all_predictions, dim=0)\n",
        "\n",
        "        # 计算训练集的精度和召回率\n",
        "        train_precision, train_recall = calculate_metrics(all_predictions, all_targets)\n",
        "\n",
        "        # 验证模型\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_targets = []\n",
        "        val_predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "\n",
        "                # 确保输出和掩码的形状一致\n",
        "                if outputs.size() != masks.size():\n",
        "                    continue  # 跳过形状不匹配的批次\n",
        "\n",
        "                val_loss += criterion(outputs, masks).item()\n",
        "\n",
        "                # 保存真实标签和预测结果\n",
        "                val_targets.append(masks)\n",
        "                val_predictions.append(outputs)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # 确保至少有一个批次的输出被保存\n",
        "        if len(val_targets) == 0:\n",
        "            print(f\"Warning: No targets collected during validation epoch {epoch + 1}.\")\n",
        "            continue\n",
        "\n",
        "        # 拼接所有的验证集真实标签和预测结果\n",
        "        val_targets = torch.cat(val_targets, dim=0)\n",
        "        val_predictions = torch.cat(val_predictions, dim=0)\n",
        "\n",
        "        # 计算验证集的精度和召回率\n",
        "        val_precision, val_recall = calculate_metrics(val_predictions, val_targets)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_loss:.4f}, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, \"\n",
        "              f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, \"\n",
        "              f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}\")\n",
        "\n",
        "# 开始训练\n",
        "train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=100, device=device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 开始训练\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 使用示例\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "vkERK7RQjQCE",
        "outputId": "d44d6204-1535-4adb-e9da-127e510711b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8006fbf0aaf1>\u001b[0m in \u001b[0;36m<cell line: 218>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;31m# 开始训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-8006fbf0aaf1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, num_epochs, device)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# 计算训练集的精度和召回率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mtrain_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# 验证模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-8006fbf0aaf1>\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(predictions, targets)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2203\u001b[0m     \"\"\"\n\u001b[0;32m-> 2204\u001b[0;31m     p, _, _, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   2205\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1787\u001b[0m     \"\"\"\n\u001b[1;32m   1788\u001b[0m     \u001b[0m_check_zero_division\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1559\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average has to be one of \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1561\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1562\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    113\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m    114\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MPXV7F6kYu_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from pycocotools.coco import COCO\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "# 创建 TensorBoard 回调\n",
        "tensorboard_callback = TensorBoard(log_dir=\"logs\")\n",
        "# COCO 数据集路径\n",
        "coco_annotation_file = '/content/drive/MyDrive/onlyvirus35/json/onlyvirus-2.json'  # 注释文件路径\n",
        "images_dir = '/content/drive/MyDrive/onlyvirus35/images'  # 图像文件夹路径\n",
        "\n",
        "# 初始化 COCO API\n",
        "coco = COCO(coco_annotation_file)\n",
        "\n",
        "# 获取图像 ID 列表\n",
        "image_ids = coco.getImgIds()\n",
        "\n",
        "# 预处理数据\n",
        "# def load_data(image_ids):\n",
        "#     X = []  # 用于存储图像\n",
        "#     y = []  # 用于存储掩膜图\n",
        "#     for img_id in image_ids:\n",
        "#         # 加载图像\n",
        "#         img_info = coco.imgs[img_id]\n",
        "#         img = cv2.imread(os.path.join(images_dir, img_info['file_name']))\n",
        "#         img = cv2.resize(img, (128, 128))  # 调整图像大小\n",
        "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # 转为灰度图\n",
        "#         X.append(img)\n",
        "\n",
        "#         # 创建掩膜图（基于标注）\n",
        "#         mask = np.zeros((128, 128), dtype=np.uint8)  # 创建一个空的概率图\n",
        "#         ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "#         anns = coco.loadAnns(ann_ids)\n",
        "\n",
        "#         # 将分割区域填入掩膜\n",
        "#         for ann in anns:\n",
        "#             if ann['area'] > 0:  # 只处理有效的标注\n",
        "#                 segmentation = coco.annToMask(ann)  # 将标注转换为掩膜\n",
        "#                 # 将分割掩膜调整到合适的大小\n",
        "#                 segmentation_resized = cv2.resize(segmentation, (128, 128), interpolation=cv2.INTER_NEAREST)  # 将分割掩膜调整为 128×128\n",
        "#                 # 合并掩膜\n",
        "#                 mask = np.maximum(mask, segmentation_resized)\n",
        "\n",
        "\n",
        "#                 # 对每张图像生成多次裁剪\n",
        "#         for _ in range(num_crops_per_image):\n",
        "#             # 随机生成裁剪的高度和宽度\n",
        "#             crop_size = np.random.randint(min_crop_size, max_crop_size + 1)\n",
        "\n",
        "#             # 随机生成裁剪的起始坐标\n",
        "#             x = np.random.randint(0, 128 - crop_size + 1)\n",
        "#             y_coord = np.random.randint(0, 128 - crop_size + 1)\n",
        "\n",
        "#             # 裁剪图像和掩膜\n",
        "#             cropped_img = img[y_coord:y_coord + crop_size, x:x + crop_size]\n",
        "#             cropped_mask = mask[y_coord:y_coord + crop_size, x:x + crop_size]\n",
        "\n",
        "#             # 调整裁剪后的图像和掩膜为固定大小（例如128x128）\n",
        "#             cropped_img_resized = cv2.resize(cropped_img, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
        "#             cropped_mask_resized = cv2.resize(cropped_mask, (128, 128), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "#             # 将生成的裁剪图像和掩膜添加到列表中\n",
        "#             X.append(cropped_img_resized)\n",
        "#             y.append(cropped_mask_resized)\n",
        "\n",
        "#     return np.array(X), np.array(y)\n",
        "# 预处理数据\n",
        "# def random_crop(image, mask, min_size=64, max_size=128):\n",
        "#     \"\"\"\n",
        "#     随机裁剪图像和掩膜\n",
        "#     :param image: 输入图像\n",
        "#     :param mask: 输入掩膜\n",
        "#     :param min_size: 最小裁剪尺寸\n",
        "#     :param max_size: 最大裁剪尺寸\n",
        "#     :return: 裁剪后的图像和掩膜\n",
        "#     \"\"\"\n",
        "#     h, w = image.shape[:2]\n",
        "\n",
        "#     # 随机选择裁剪的宽和高\n",
        "#     crop_height = np.random.randint(min_size, max_size)\n",
        "#     crop_width = np.random.randint(min_size, max_size)\n",
        "\n",
        "#     # 确保裁剪区域不超出原图像的边界\n",
        "#     top = np.random.randint(0, h - crop_height + 1)\n",
        "#     left = np.random.randint(0, w - crop_width + 1)\n",
        "\n",
        "#     # 进行裁剪\n",
        "#     cropped_img = image[top:top + crop_height, left:left + crop_width]\n",
        "#     cropped_mask = mask[top:top + crop_height, left:left + crop_width]\n",
        "\n",
        "#     return cropped_img, cropped_mask\n",
        "\n",
        "\n",
        "\n",
        "def load_data(image_ids,target_size=(114, 114)):\n",
        "    X = []  # 用于存储图像\n",
        "    y = []  # 用于存储掩膜图\n",
        "    for img_id in image_ids:\n",
        "        # 加载图像\n",
        "        img_info = coco.imgs[img_id]\n",
        "        img_path = os.path.join(images_dir, img_info['file_name'])\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Warning: Image at {img_path} could not be loaded.\")\n",
        "            continue  # 如果图像加载失败，跳过该图像\n",
        "\n",
        "        # 检查通道数，如果是彩色图像则转换为灰度图\n",
        "        if img.ndim == 3 and img.shape[2] == 3:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # 转为灰度图\n",
        "        img_resized = cv2.resize(img, target_size)\n",
        "        X.append(img_resized)\n",
        "\n",
        "        # 创建掩膜图\n",
        "        mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        anns = coco.loadAnns(ann_ids)\n",
        "        for ann in anns:\n",
        "            if ann['area'] > 0:\n",
        "                segmentation = coco.annToMask(ann)\n",
        "                mask = np.maximum(mask, segmentation)\n",
        "                # 调整掩膜大小\n",
        "        mask_resized = cv2.resize(mask, target_size)\n",
        "        y.append(mask_resized)\n",
        "\n",
        "    X_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    print(f\"Loaded{len(X)} images and {len(y)} masks\")\n",
        "\n",
        "\n",
        "    return X,y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X, y = load_data(image_ids)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"X length: {len(X)}, y length: {len(y)}\")\n",
        "\n",
        "\n",
        "X, y = load_data(image_ids)\n",
        "assert len(X) == len(y), \"X and y must have the same length.\"\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# 假设你有一个列表 image_ids，包含所有图像的 ID\n",
        "random.seed(42)  # 设置随机种子以确保结果可重复\n",
        "all_image_ids = list(image_ids)  # 将 image_ids 转换为列表\n",
        "\n",
        "# 随机选择 18 张作为训练集\n",
        "train_image_ids = random.sample(all_image_ids, 18)\n",
        "\n",
        "# 从剩余图像中选择 17 张作为测试集\n",
        "remaining_ids = list(set(all_image_ids) - set(train_image_ids))  # 获取剩余图像的 ID\n",
        "test_image_ids = random.sample(remaining_ids, 17)\n",
        "\n",
        "# 确保选择后的 ID 数量正确\n",
        "assert len(train_image_ids) == 18, \"训练集应该有 18 张图片\"\n",
        "assert len(test_image_ids) == 17, \"测试集应该有 17 张图片\"\n",
        "# 加载训练数据\n",
        "X_train, y_train = load_data(train_image_ids)\n",
        "\n",
        "# 加载测试数据\n",
        "X_test, y_test = load_data(test_image_ids)\n",
        "\n",
        "# 确保训练集和测试集的长度一致\n",
        "assert len(X_train) == len(y_train), \"训练集的 X 和 y 长度必须一致\"\n",
        "assert len(X_test) == len(y_test), \"测试集的 X 和 y 长度必须一致\"\n",
        "# 训练模型\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_model(input_shape):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "    # 第一个卷积层，使用7x7滤波器，输出122x122\n",
        "    model.add(layers.Conv2D(64, (7, 7), padding='same'))  # 128x128 -> 128x128\n",
        "\n",
        "    # 使用边缘填充，使输出大小为122x122\n",
        "    model.add(layers.Lambda(lambda x: tf.image.resize(x, (122, 122))))\n",
        "\n",
        "    # 第二个卷积层，使用3x3滤波器，输出120x120\n",
        "    model.add(layers.Conv2D(16, (3, 3), padding='same'))  # 122x122 -> 122x122\n",
        "\n",
        "    # 使用边缘填充，使输出大小为120x120\n",
        "    model.add(layers.Lambda(lambda x: tf.image.resize(x, (120, 120))))\n",
        "\n",
        "    # 第三个卷积层，使用7x7滤波器，输出114x114\n",
        "    model.add(layers.Conv2D(1, (7, 7), padding='same', activation='sigmoid'))  # 120x120 -> 120x120\n",
        "\n",
        "    # 使用边缘填充，使输出大小为114x114\n",
        "    model.add(layers.Lambda(lambda x: tf.image.resize(x, (114, 114))))\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-8),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy',\n",
        "                            tf.keras.metrics.Precision(),\n",
        "                            tf.keras.metrics.Recall(),\n",
        "                            tf.keras.metrics.AUC()])\n",
        "\n",
        "    return model\n",
        "\n",
        "# 假设输入图像的形状为 (128, 128, 1)\n",
        "input_shape = (128, 128, 1)\n",
        "model = create_model(input_shape)\n",
        "\n",
        "# 确保掩膜的形状符合模型输入要求\n",
        "X_train = np.expand_dims(X_train, axis=-1)  # (num_samples, 128, 128, 1)\n",
        "y_train = np.expand_dims(y_train, axis=-1)  # (num_samples, 128, 128, 1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "y_test = np.expand_dims(y_test, axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 编译模型\n",
        "input_shape = (128, 128, 1)  # 输入图像的形状\n",
        "model = create_model(input_shape)\n",
        "model.fit(X_train, y_train, batch_size=8, epochs=100, validation_data=(X_test, y_test), callbacks=[tensorboard_callback])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# 训练模型\n",
        "# 评估模型\n",
        "\n",
        "\n",
        "results = model.evaluate(X_test, y_test)\n",
        "\n",
        "# 解包所有返回的值\n",
        "loss = results[0]\n",
        "accuracy = results[1]\n",
        "precision = results[2]\n",
        "recall = results[3]\n",
        "auc_score = results[4]\n",
        "\n",
        "# 输出评估结果\n",
        "print(f'Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, AUC: {auc_score:.4f}')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 进行预测\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)  # 将概率转换为二进制标签\n",
        "y_true = y_test.flatten()      # 将真实标签展平\n",
        "y_pred_classes = y_pred_classes.flatten()  # 将预测标签展平\n",
        "\n",
        "# 打印分类报告\n",
        "print(classification_report(y_true, y_pred_classes, target_names=['Background', 'Foreground']))\n",
        "# # 打印分类报告\n",
        "# print(classification_report(y_test.flatten(), y_pred_classes.flatten(), target_names=['Background', 'Foreground']))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KW6sEhAqv7kU",
        "outputId": "953a6537-4d77-495d-905c-022ee11d5a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Loaded35 images and 35 masks\n",
            "X length: 35, y length: 35\n",
            "Loaded35 images and 35 masks\n",
            "Loaded18 images and 18 masks\n",
            "Loaded17 images and 17 masks\n",
            "Epoch 1/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4s/step - accuracy: 0.6703 - auc_11: 0.6205 - loss: 4.6830 - precision_11: 0.1564 - recall_11: 0.5621 - val_accuracy: 0.9462 - val_auc_11: 0.4994 - val_loss: 0.8646 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8814 - auc_11: 0.4978 - loss: 1.9059 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9463 - val_auc_11: 0.4996 - val_loss: 0.8644 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 3/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8704 - auc_11: 0.4988 - loss: 2.0867 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4997 - val_loss: 0.8643 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 4/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8899 - auc_11: 0.4984 - loss: 1.7711 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4997 - val_loss: 0.8643 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 5/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8664 - auc_11: 0.4989 - loss: 2.1529 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4998 - val_loss: 0.8643 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 6/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8802 - auc_11: 0.4992 - loss: 1.9299 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8643 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 7/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8953 - auc_11: 0.4992 - loss: 1.6875 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8643 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 8/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8691 - auc_11: 0.4988 - loss: 2.1099 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8643 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 9/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8710 - auc_11: 0.4989 - loss: 2.0789 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8643 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 10/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8853 - auc_11: 0.4993 - loss: 1.8490 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8643 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 11/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8565 - auc_11: 0.4990 - loss: 2.3129 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8643 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 12/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8707 - auc_11: 0.4992 - loss: 2.0846 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 13/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8729 - auc_11: 0.4994 - loss: 2.0483 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 14/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8614 - auc_11: 0.4994 - loss: 2.2337 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 15/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8742 - auc_11: 0.4995 - loss: 2.0278 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 16/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8894 - auc_11: 0.4992 - loss: 1.7831 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 17/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8584 - auc_11: 0.4996 - loss: 2.2817 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 18/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8725 - auc_11: 0.4996 - loss: 2.0551 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 19/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8573 - auc_11: 0.4995 - loss: 2.3006 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 20/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8932 - auc_11: 0.4997 - loss: 1.7221 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.4999 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 21/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8993 - auc_11: 0.4996 - loss: 1.6230 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 22/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8710 - auc_11: 0.4998 - loss: 2.0793 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 23/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8772 - auc_11: 0.4996 - loss: 1.9787 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 24/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8725 - auc_11: 0.4996 - loss: 2.0554 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 25/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8647 - auc_11: 0.4997 - loss: 2.1807 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 26/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8811 - auc_11: 0.4997 - loss: 1.9164 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 27/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8898 - auc_11: 0.4997 - loss: 1.7770 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 28/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8800 - auc_11: 0.4998 - loss: 1.9337 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 29/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8817 - auc_11: 0.4997 - loss: 1.9065 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 30/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8597 - auc_11: 0.4997 - loss: 2.2621 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 31/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8698 - auc_11: 0.4998 - loss: 2.0986 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 32/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8781 - auc_11: 0.4998 - loss: 1.9655 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 33/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8949 - auc_11: 0.4998 - loss: 1.6947 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 34/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8727 - auc_11: 0.4999 - loss: 2.0515 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 35/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8904 - auc_11: 0.4998 - loss: 1.7666 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 36/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8801 - auc_11: 0.4999 - loss: 1.9320 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 37/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8638 - auc_11: 0.4999 - loss: 2.1952 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 38/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8747 - auc_11: 0.4999 - loss: 2.0197 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 39/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8709 - auc_11: 0.4999 - loss: 2.0808 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 40/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8840 - auc_11: 0.4999 - loss: 1.8705 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 41/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8736 - auc_11: 0.4999 - loss: 2.0368 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 42/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8863 - auc_11: 0.4999 - loss: 1.8328 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 43/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8903 - auc_11: 0.4999 - loss: 1.7675 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 44/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8776 - auc_11: 0.5000 - loss: 1.9736 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 45/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8965 - auc_11: 0.4999 - loss: 1.6679 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 46/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8827 - auc_11: 0.5000 - loss: 1.8903 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 47/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8867 - auc_11: 0.4999 - loss: 1.8259 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 48/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8572 - auc_11: 0.5000 - loss: 2.3017 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 49/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8866 - auc_11: 0.5000 - loss: 1.8277 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 50/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8887 - auc_11: 0.4999 - loss: 1.7945 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 51/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8830 - auc_11: 0.5000 - loss: 1.8858 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 52/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8583 - auc_11: 0.5000 - loss: 2.2834 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 53/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8726 - auc_11: 0.5000 - loss: 2.0531 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 54/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8629 - auc_11: 0.5000 - loss: 2.2102 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 55/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8730 - auc_11: 0.5000 - loss: 2.0476 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 56/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8961 - auc_11: 0.5000 - loss: 1.6739 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 57/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8941 - auc_11: 0.5000 - loss: 1.7064 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 58/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8859 - auc_11: 0.5000 - loss: 1.8390 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 59/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8583 - auc_11: 0.5000 - loss: 2.2833 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 60/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8753 - auc_11: 0.5000 - loss: 2.0091 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 61/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8779 - auc_11: 0.5000 - loss: 1.9677 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 62/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8654 - auc_11: 0.5000 - loss: 2.1693 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 63/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8744 - auc_11: 0.5000 - loss: 2.0246 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 64/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8698 - auc_11: 0.5000 - loss: 2.0990 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 65/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8864 - auc_11: 0.5000 - loss: 1.8307 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 66/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8648 - auc_11: 0.5000 - loss: 2.1792 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 67/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8771 - auc_11: 0.5000 - loss: 1.9809 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 68/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8728 - auc_11: 0.5000 - loss: 2.0501 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 69/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8851 - auc_11: 0.5000 - loss: 1.8523 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 70/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8697 - auc_11: 0.5000 - loss: 2.1000 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 71/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8780 - auc_11: 0.5000 - loss: 1.9660 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 72/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8722 - auc_11: 0.5000 - loss: 2.0599 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 73/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8837 - auc_11: 0.5000 - loss: 1.8746 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 74/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8718 - auc_11: 0.5000 - loss: 2.0668 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 75/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8920 - auc_11: 0.5000 - loss: 1.7409 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 76/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8762 - auc_11: 0.5000 - loss: 1.9962 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 77/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8841 - auc_11: 0.5000 - loss: 1.8673 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 78/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8766 - auc_11: 0.5000 - loss: 1.9892 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 79/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8838 - auc_11: 0.5000 - loss: 1.8723 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 80/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8661 - auc_11: 0.5000 - loss: 2.1582 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 81/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8877 - auc_11: 0.5000 - loss: 1.8095 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 82/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8881 - auc_11: 0.5000 - loss: 1.8030 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 83/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8956 - auc_11: 0.5000 - loss: 1.6834 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 84/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8708 - auc_11: 0.5000 - loss: 2.0822 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 85/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8724 - auc_11: 0.5000 - loss: 2.0565 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 86/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8716 - auc_11: 0.5000 - loss: 2.0702 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 87/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8847 - auc_11: 0.5000 - loss: 1.8581 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 88/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8605 - auc_11: 0.5000 - loss: 2.2486 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 89/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8620 - auc_11: 0.5000 - loss: 2.2235 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 90/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8750 - auc_11: 0.5000 - loss: 2.0148 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 91/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8876 - auc_11: 0.5000 - loss: 1.8113 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 92/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8689 - auc_11: 0.5000 - loss: 2.1124 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 93/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8633 - auc_11: 0.5000 - loss: 2.2036 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 94/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8674 - auc_11: 0.5000 - loss: 2.1376 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 95/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8818 - auc_11: 0.5000 - loss: 1.9047 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 96/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8797 - auc_11: 0.5000 - loss: 1.9395 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 97/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8909 - auc_11: 0.5000 - loss: 1.7585 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 98/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8816 - auc_11: 0.5000 - loss: 1.9090 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 99/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8619 - auc_11: 0.5000 - loss: 2.2259 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n",
            "Epoch 100/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8799 - auc_11: 0.5000 - loss: 1.9362 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00 - val_accuracy: 0.9464 - val_auc_11: 0.5000 - val_loss: 0.8642 - val_precision_11: 0.0000e+00 - val_recall_11: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_33 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m3,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lambda_15 (\u001b[38;5;33mLambda\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_34 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m9,232\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lambda_16 (\u001b[38;5;33mLambda\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_35 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m785\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lambda_17 (\u001b[38;5;33mLambda\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m114\u001b[0m, \u001b[38;5;34m114\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lambda_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,232</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lambda_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">785</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lambda_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">114</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">114</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,436\u001b[0m (103.27 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,436</span> (103.27 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,217\u001b[0m (51.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,217</span> (51.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m13,219\u001b[0m (51.64 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,219</span> (51.64 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.9464 - auc_11: 0.5000 - loss: 0.8642 - precision_11: 0.0000e+00 - recall_11: 0.0000e+00\n",
            "Loss: 0.8642, Accuracy: 0.9464, Precision: 0.0000, Recall: 0.0000, AUC: 0.5000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Background       0.95      1.00      0.97    209086\n",
            "  Foreground       0.00      0.00      0.00     11846\n",
            "\n",
            "    accuracy                           0.95    220932\n",
            "   macro avg       0.47      0.50      0.49    220932\n",
            "weighted avg       0.90      0.95      0.92    220932\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用逻辑回归进行像素分类\n",
        "def logistic_regression_per_pixel(features, labels):\n",
        "    # 展平特征和标签以用于逻辑回归\n",
        "    features = features.reshape(-1, features.shape[-1])  # (num_pixels, num_features)\n",
        "    labels = labels.flatten()  # (num_pixels,)\n",
        "\n",
        "    # 使用逻辑回归进行训练\n",
        "    log_reg = LogisticRegression(max_iter=100)\n",
        "    log_reg.fit(features, labels)\n",
        "\n",
        "    return log_reg\n",
        "\n",
        "# 模型训练流程\n",
        "def train_model(model, train_loader):\n",
        "    model.eval()  # 模型进入评估模式\n",
        "    for images, masks in train_loader:\n",
        "        features = model(images)  # 提取特征\n",
        "        features = features.permute(0, 2, 3, 1)  # 转换为 (batch_size, height, width, num_features)\n",
        "\n",
        "        # 对每个图像的每个像素应用逻辑回归\n",
        "        for i in range(features.shape[0]):\n",
        "            log_reg = logistic_regression_per_pixel(features[i].detach().numpy(), masks[i].numpy())\n",
        "\n",
        "            # 可以进一步使用log_reg来预测和评估\n"
      ],
      "metadata": {
        "id": "Vn9yNHXJivkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def predict_pixel_probabilities(log_reg, features):\n",
        "    # 使用逻辑回归计算后验概率\n",
        "    features = features.reshape(-1, features.shape[-1])\n",
        "    probabilities = sigmoid(log_reg.decision_function(features))\n",
        "    return probabilities.reshape(120, 120)  # 恢复为原始形状\n"
      ],
      "metadata": {
        "id": "8l6Oscnsi0jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from skimage.morphology import remove_small_objects\n",
        "\n",
        "def post_process(prob_map, threshold=0.5, min_size=2.5):\n",
        "    # 阈值化\n",
        "    binary_map = prob_map > threshold\n",
        "\n",
        "    # 移除小于2.5个像素的小区域\n",
        "    processed_map = remove_small_objects(binary_map, min_size=min_size)\n",
        "\n",
        "    return processed_map\n",
        "\n",
        "# 例子：对一个概率图进行后处理\n",
        "prob_map = np.random.rand(120, 120)  # 假设生成了一个概率图\n",
        "processed_map = post_process(prob_map)\n"
      ],
      "metadata": {
        "id": "UBXoDk7Ji5S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "def evaluate_model(model, log_reg, val_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, masks in val_loader:\n",
        "        features = model(images)\n",
        "        features = features.permute(0, 2, 3, 1)\n",
        "\n",
        "        for i in range(features.shape[0]):\n",
        "            probs = predict_pixel_probabilities(log_reg, features[i].detach().numpy())\n",
        "            preds = probs > 0.5\n",
        "            all_preds.append(preds.flatten())\n",
        "            all_labels.append(masks[i].flatten())\n",
        "\n",
        "    accuracy = accuracy_score(np.concatenate(all_labels), np.concatenate(all_preds))\n",
        "    auc = roc_auc_score(np.concatenate(all_labels), np.concatenate(all_preds))\n",
        "\n",
        "    print(f'Accuracy: {accuracy}, AUC: {auc}')\n"
      ],
      "metadata": {
        "id": "nIq9614ei737"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}